version: '3.8'

services:
  # 1. Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: ragcode-qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped

  # 2. Local AI (Ollama) - Containerized but using Host Models
  ollama:
    image: ollama/ollama:latest
    container_name: ragcode-ollama
    ports:
      - "11434:11434"
    volumes:
      # MAPARE CRITICĂ: Folosește modelele descărcate local pe host
      # Astfel nu le descarci de două ori (o dată local, o dată în docker)
      - ${HOME}/.ollama:/root/.ollama
    # Activează suportul GPU dacă este disponibil (necesită nvidia-container-toolkit)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    dns:
      - 8.8.8.8
    restart: unless-stopped

  # 3. RagCode MCP Server (Optional - if you want to run the binary in docker too)
  # ragcode:
  #   build: .
  #   container_name: ragcode-mcp
  #   environment:
  #     - QDRANT_URL=http://qdrant:6333
  #     - OLLAMA_BASE_URL=http://ollama:11434
  #   depends_on:
  #     - qdrant
  #     - ollama
  #   volumes:
  #     - ./:/app/workspace # Map current directory to index it

volumes:
  qdrant_data:
